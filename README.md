# ollama_test.py
A Python script demonstrating how to interact with locally deployed Large Language Models (LLMs) using the Ollama API. This menu-driven tool allows users to:  Send a single prompt and receive a model-generated response.  Engage in an interactive chatbot session.  Submit multiple questions for batch Q&amp;A.  Summarize a block of text. 
The script uses the official ollama Python library to communicate with the local Ollama server and supports any installed model (e.g., Llama 3.2, Mistral, Gemma).
Features
Single Prompt/Response: Quickly ask questions and get instant answers from your LLM.

Interactive Chatbot: Chat with the model in a conversational loop.

Batch Q&A: Enter multiple questions and receive all answers in sequence.

Text Summarization: Paste any text and get a concise summary generated by the model.

Menu-Driven Interface: Simple command-line menu for easy navigation between features.

Well-Commented Code: Each function is clearly documented for easy understanding and modification.

Requirements
Python 3.7 or higher

Ollama installed and running locally

At least one LLM model downloaded via Ollama (e.g., llama3.2)

Python package: ollama (install with pip install ollama)
Usage:
Ensure Ollama is running and your desired model is available.
Install the Python package:
pip install ollama
Run the script:
python ollana_test.py
