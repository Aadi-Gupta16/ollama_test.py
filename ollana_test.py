import ollama # type: ignore

# Change this to your installed model (e.g., 'llama3.2', 'mistral', etc.)
MODEL_NAME = 'llama3.2'

def single_prompt():
    """
    Function to take a single user input prompt,
    send it to the model, and print the response.
    """
    prompt = input("Enter your question: ")
    # Send the prompt to the Ollama model and get the response
    response = ollama.generate(model=MODEL_NAME, prompt=prompt)
    print("\nModel response:\n", response['response'])

def interactive_chatbot():
    """
    Function to create an interactive chatbot session,
    where user can type multiple prompts and get responses
    until they type 'exit' or 'quit'.
    """
    print("\nType 'exit' or 'quit' to end the chat.")
    while True:
        prompt = input("You: ")
        if prompt.lower() in ['exit', 'quit']:
            print("Chat ended.\n")
            break
        # Generate a response for each user input
        response = ollama.generate(model=MODEL_NAME, prompt=prompt)
        print("Ollama:", response['response'])

def batch_qa():
    """
    Function to collect multiple questions from the user,
    send each question to the model, and print all answers.
    The user types 'done' to finish entering questions.
    """
    print("\nEnter your questions one by one. Type 'done' when finished.")
    questions = []
    while True:
        q = input("Question: ")
        if q.lower() == 'done':
            break
        questions.append(q)
    print("\nBatch Q&A Results:")
    for q in questions:
        # Generate and print response for each question
        response = ollama.generate(model=MODEL_NAME, prompt=q)
        print(f"Q: {q}\nA: {response['response']}\n")

def summarize_text():
    """
    Function to accept multiline text input from the user,
    then send the text to the model with a summarization prompt,
    and print the summary generated by the model.
    The user types 'end' on a new line to finish input.
    """
    print("\nPaste the text you want summarized. Type 'end' on a new line to finish.")
    lines = []
    while True:
        line = input()
        if line.strip().lower() == 'end':
            break
        lines.append(line)
    text = "\n".join(lines)
    # Prepare summarization prompt
    prompt = f"Summarize the following text:\n{text}"
    # Get summary from the model
    response = ollama.generate(model=MODEL_NAME, prompt=prompt)
    print("\nSummary:\n", response['response'])

def main():
    """
    Main function to display a menu and allow the user
    to select which functionality to run.
    Loops until user chooses to exit.
    """
    while True:
        print("\n--- Ollama Multi-Tool ---")
        print("1. Single Prompt/Response")
        print("2. Interactive Chatbot")
        print("3. Batch Q&A")
        print("4. Text Summarization")
        print("5. Exit")
        choice = input("Choose an option (1-5): ")
        if choice == '1':
            single_prompt()
        elif choice == '2':
            interactive_chatbot()
        elif choice == '3':
            batch_qa()
        elif choice == '4':
            summarize_text()
        elif choice == '5':
            print("Goodbye!")
            break
        else:
            print("Invalid choice. Please select a valid option.")

# Entry point of the script
if __name__ == "__main__":
    main()
